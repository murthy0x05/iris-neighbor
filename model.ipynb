{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330182b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74124ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path(\"dataset\") / \"iris.data\"\n",
    "MODEL_PATH = pathlib.Path(\"model.joblib\")\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6470fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=DATA_PATH):\n",
    "    \"\"\"Load local iris.data file into a DataFrame and return X, y, df.\"\"\"\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df.columns = ['sepal_length','sepal_width','petal_length','petal_width','class']\n",
    "    # Ensure class is string\n",
    "    df['class'] = df['class'].astype(str)\n",
    "    X = df.iloc[:, :4].values\n",
    "    y = df['class'].values\n",
    "    return X, y, df\n",
    "\n",
    "def build_pipeline():\n",
    "    \"\"\"Return a sklearn Pipeline with scaler + KNN placeholder.\"\"\"\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "def tune_hyperparameters(pipe, X_train, y_train, cv=5, n_jobs=-1):\n",
    "    \"\"\"Grid search for best hyperparameters for KNN inside the pipeline.\"\"\"\n",
    "    param_grid = {\n",
    "        'knn__n_neighbors': [3, 5, 7, 9],\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "        'knn__p': [1, 2]  # 1 = Manhattan, 2 = Euclidean\n",
    "    }\n",
    "    gs = GridSearchCV(pipe, param_grid, cv=cv, n_jobs=n_jobs, verbose=1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    return gs\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate a trained model on test data and print metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nConfusion matrix:\\n\", cm)\n",
    "    print(\"\\nClassification report:\\n\", report)\n",
    "    return acc, cm, report\n",
    "\n",
    "def save_model(model, path=MODEL_PATH):\n",
    "    \"\"\"Save the model (pipeline) using joblib.\"\"\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"Saved model to {path}\")\n",
    "\n",
    "def predict_sample(sample, model_path=MODEL_PATH):\n",
    "    \"\"\"\n",
    "    Load saved pipeline and predict class for a single sample.\n",
    "    sample: iterable with 4 elements [sepal_length, sepal_width, petal_length, petal_width]\n",
    "    \"\"\"\n",
    "    sample_arr = np.array(sample).reshape(1, -1)\n",
    "    model = joblib.load(model_path)\n",
    "    return model.predict(sample_arr)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7db32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    print(\"Loading data...\")\n",
    "    X, y, df = load_data()\n",
    "    print(\"Rows, cols:\", df.shape)\n",
    "    print(\"Class distribution:\")\n",
    "    print(df['class'].value_counts())\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head(), \"\\n\")\n",
    "\n",
    "    # Split\n",
    "    print(\"Splitting into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\\n\")\n",
    "\n",
    "    # Build pipeline\n",
    "    print(\"Building pipeline (StandardScaler + KNN)...\")\n",
    "    pipe = build_pipeline()\n",
    "\n",
    "    # Tune\n",
    "    print(\"Starting GridSearchCV to find best hyperparameters for KNN...\")\n",
    "    gs = tune_hyperparameters(pipe, X_train, y_train)\n",
    "    print(\"\\nBest params found:\")\n",
    "    pprint(gs.best_params_)\n",
    "\n",
    "    # Best model\n",
    "    best_model = gs.best_estimator_\n",
    "    print(\"\\nEvaluating best model on test set...\")\n",
    "    evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "    # Save\n",
    "    save_model(best_model)\n",
    "\n",
    "    # Example prediction\n",
    "    example = [5.1, 3.5, 1.4, 0.2]  # typical Iris-setosa sample\n",
    "    pred = predict_sample(example)\n",
    "    print(f\"\\nExample prediction for {example} -> {pred}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
